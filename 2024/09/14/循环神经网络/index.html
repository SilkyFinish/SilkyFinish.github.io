<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 7.3.0">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.2/css/all.min.css" integrity="sha256-XOqroi11tY4EFQMR9ZYwZWKj5ZXiftSx36RRuC3anlA=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"silkyfinish.github.io","root":"/","images":"/images","scheme":"Pisces","darkmode":false,"version":"8.20.0","exturl":false,"sidebar":{"position":"left","width_expanded":320,"width_dual_column":240,"display":"post","padding":18,"offset":12},"copycode":{"enable":false,"style":null},"fold":{"enable":false,"height":500},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"i18n":{"placeholder":"Searching...","empty":"We didn't find any results for the search: ${query}","hits_time":"${hits} results found in ${time} ms","hits":"${hits} results found"}}</script><script src="/js/config.js"></script>

    <meta name="description" content="第八章 循环神经网络 8.1 序列模型 1.很多数据具有时序结构 2.统计工具 - 在时间t观察到\(x_t\),得到T个不独立的随机变量（\(x_1,...x_T\))~p(x） - p(x）&#x3D;p（x1)p(x2|x1)p(x3|x1,x2)...p(xT|x1,...xT-1) - p(x）&#x3D;p（xT)p(xT-1|xT)p(xT-2|xT-1,xT-2)...p(x1|x1,...x">
<meta property="og:type" content="article">
<meta property="og:title" content="循环神经网络">
<meta property="og:url" content="https://silkyfinish.github.io/2024/09/14/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/index.html">
<meta property="og:site_name" content="SilkyFinish">
<meta property="og:description" content="第八章 循环神经网络 8.1 序列模型 1.很多数据具有时序结构 2.统计工具 - 在时间t观察到\(x_t\),得到T个不独立的随机变量（\(x_1,...x_T\))~p(x） - p(x）&#x3D;p（x1)p(x2|x1)p(x3|x1,x2)...p(xT|x1,...xT-1) - p(x）&#x3D;p（xT)p(xT-1|xT)p(xT-2|xT-1,xT-2)...p(x1|x1,...x">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2024-09-14T09:29:28.000Z">
<meta property="article:modified_time" content="2024-09-14T09:58:03.235Z">
<meta property="article:author" content="SilkyFinish">
<meta name="twitter:card" content="summary">


<link rel="canonical" href="https://silkyfinish.github.io/2024/09/14/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"en","comments":true,"permalink":"https://silkyfinish.github.io/2024/09/14/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/","path":"2024/09/14/循环神经网络/","title":"循环神经网络"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>循环神经网络 | SilkyFinish</title>
  








  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">SilkyFinish</p>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">SilkyFinish的个人博客</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="Search" role="button">
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>About</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a></li>
  </ul>
</nav>




</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#%E7%AC%AC%E5%85%AB%E7%AB%A0-%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="nav-number">1.</span> <span class="nav-text">第八章 循环神经网络</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%BA%8F%E5%88%97%E6%A8%A1%E5%9E%8B"><span class="nav-number">1.1.</span> <span class="nav-text">8.1 序列模型</span></a></li></ol></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="SilkyFinish"
      src="/images/avatar.jpg">
  <p class="site-author-name" itemprop="name">SilkyFinish</p>
  <div class="site-description" itemprop="description">冲天香阵透长安，满城尽带黄金甲</div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">4</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
        <span class="site-state-item-count">1</span>
        <span class="site-state-item-name">categories</span>
      </div>
      <div class="site-state-item site-state-tags">
        <span class="site-state-item-count">1</span>
        <span class="site-state-item-name">tags</span>
      </div>
  </nav>
</div>

        </div>
      </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="en">
    <link itemprop="mainEntityOfPage" href="https://silkyfinish.github.io/2024/09/14/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="SilkyFinish">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="SilkyFinish">
      <meta itemprop="description" content="冲天香阵透长安，满城尽带黄金甲">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="循环神经网络 | SilkyFinish">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          循环神经网络
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>
      

      <time title="Created: 2024-09-14 17:29:28 / Modified: 17:58:03" itemprop="dateCreated datePublished" datetime="2024-09-14T17:29:28+08:00">2024-09-14</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody"><h1 id="第八章-循环神经网络">第八章 循环神经网络</h1>
<h2 id="序列模型">8.1 序列模型</h2>
<p>1.很多数据具有时序结构 2.统计工具 - 在时间t观察到<span
class="math inline">\(x_t\)</span>,得到T个不独立的随机变量（<span
class="math inline">\(x_1,...x_T\)</span>)~p(<strong>x</strong>） -
p(<strong>x</strong>）=p（x1)p(x2|x1)p(x3|x1,x2)...p(xT|x1,...xT-1) -
p(<strong>x</strong>）=p（xT)p(xT-1|xT)p(xT-2|xT-1,xT-2)...p(x1|x1,...xT-1)
-
对条件概率建模：p(xt|x1...xt-1)=p(xt|f(x1...xt-1)),对见过的数据建模，也称自回归模型
3.方案A 马尔可夫假设 - 假设当前数据只和<span
class="math inline">\(\tau\)</span>个过去数据点相关 -
p(xt|x1...xt-1)=p(xt|xt-<span
class="math inline">\(\tau\)</span>...xt-1)=p(xt|f(xt-<span
class="math inline">\(\tau\)</span>...xt-1)),例如在过去数据上训练MLP模型
- <span class="math inline">\(\tau\)</span>=1时，得到一阶马尔可夫模型，
p(<strong>x</strong>）=<span
class="math inline">\(\Pi\)</span>p(xt|xt-1) 4.方案B 潜变量模型 -
潜变量ht=f(x1...xt-1)-&gt;xt=p（xt|ht） - ![[Pasted image
20240806115112.png]] ## 8.2 文本预处理 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> collections</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br></pre></td></tr></table></figure> ###
将数据集读取到由文本行组成的列表中 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">d2l.DATA_HUB[<span class="string">&#x27;time_machine&#x27;</span>] = (d2l.DATA_URL + <span class="string">&#x27;timemachine.txt&#x27;</span>,</span><br><span class="line">                                <span class="string">&#x27;090b5e7e70c295757f55df93cb0a180b9691891a&#x27;</span>)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;读取一本书&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">read_time_machine</span>():  </span><br><span class="line">    <span class="string">&quot;&quot;&quot;Load the time machine dataset into a list of text lines.&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(d2l.download(<span class="string">&#x27;time_machine&#x27;</span>), <span class="string">&#x27;r&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">        lines = f.readlines()</span><br><span class="line">    <span class="keyword">return</span> [re.sub(<span class="string">&#x27;[^A-Za-z]+&#x27;</span>, <span class="string">&#x27; &#x27;</span>, line).strip().lower() <span class="keyword">for</span> line <span class="keyword">in</span> lines]</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;把不是字母和空格的都变成空格&#x27;&#x27;&#x27;</span></span><br><span class="line">lines = read_time_machine()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;</span></span><br><span class="line"><span class="string">print(lines[0])</span></span><br><span class="line"><span class="string">print(lines[10])</span></span><br></pre></td></tr></table></figure> text lines: 3221 the
time machine by h g wells twinkled and his usually pale face was flushed
and animated the ### 每个文本序列被拆分成一个标记列表
文本行列表lines-文本序列line-词元列表tokens-词元token <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">tokenize</span>(<span class="params">lines, token=<span class="string">&#x27;word&#x27;</span></span>):  </span><br><span class="line">    <span class="string">&quot;&quot;&quot;将文本行拆分为单词或字符标记。&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">if</span> token == <span class="string">&#x27;word&#x27;</span>:</span><br><span class="line">        <span class="keyword">return</span> [line.split() <span class="keyword">for</span> line <span class="keyword">in</span> lines]</span><br><span class="line">    <span class="keyword">elif</span> token == <span class="string">&#x27;char&#x27;</span>:</span><br><span class="line">        <span class="keyword">return</span> [<span class="built_in">list</span>(line) <span class="keyword">for</span> line <span class="keyword">in</span> lines]</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;错误：未知令牌类型：&#x27;</span> + token)</span><br><span class="line"></span><br><span class="line">tokens = tokenize(lines)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">11</span>):</span><br><span class="line">    <span class="built_in">print</span>(tokens[i])</span><br></pre></td></tr></table></figure>
['the', 'time', 'machine', 'by', 'h', 'g', 'wells'] [] [] [] [] ['i'] []
[] ['the', 'time', 'traveller', 'for', 'so', 'it', 'will', 'be',
'convenient', 'to', 'speak', 'of', 'him'] ['was', 'expounding', 'a',
'recondite', 'matter', 'to', 'us', 'his', 'grey', 'eyes', 'shone',
'and'] ['twinkled', 'and', 'his', 'usually', 'pale', 'face', 'was',
'flushed', 'and', 'animated', 'the'] ###
构建一个字典，通常也叫做_词表_（vocabulary），用来将字符串标记映射到从0开始的数字索引中
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Vocab</span>:  </span><br><span class="line">    <span class="string">&quot;&quot;&quot;文本词表&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, tokens=<span class="literal">None</span>, min_freq=<span class="number">0</span>, reserved_tokens=<span class="literal">None</span></span>):</span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;如果某个词出现次数小于min_freq，就不要了；保存那些被保留的词元， 例如：填充词元（“&lt;pad&gt;”）； 序列开始词元（“&lt;bos&gt;”）； 序列结束词元（“&lt;eos&gt;”）&#x27;&#x27;&#x27;</span></span><br><span class="line">        <span class="keyword">if</span> tokens <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            tokens = []</span><br><span class="line">        <span class="keyword">if</span> reserved_tokens <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            reserved_tokens = []</span><br><span class="line">        counter = count_corpus(tokens)</span><br><span class="line">        <span class="variable language_">self</span>.token_freqs = <span class="built_in">sorted</span>(counter.items(), key=<span class="keyword">lambda</span> x: x[<span class="number">1</span>],</span><br><span class="line">                                  reverse=<span class="literal">True</span>)<span class="string">&#x27;&#x27;&#x27;频率排序&#x27;&#x27;&#x27;</span></span><br><span class="line">        <span class="variable language_">self</span>.unk<span class="string">&#x27;&#x27;&#x27;unknown记为0&#x27;&#x27;&#x27;</span>, uniq_tokens = <span class="number">0</span>, [<span class="string">&#x27;&lt;unk&gt;&#x27;</span>] + reserved_tokens</span><br><span class="line">        uniq_tokens += [</span><br><span class="line">            token <span class="keyword">for</span> token, freq <span class="keyword">in</span> <span class="variable language_">self</span>.token_freqs</span><br><span class="line">            <span class="keyword">if</span> freq &gt;= min_freq <span class="keyword">and</span> token <span class="keyword">not</span> <span class="keyword">in</span> uniq_tokens]</span><br><span class="line">        <span class="variable language_">self</span>.idx_to_token, <span class="variable language_">self</span>.token_to_idx = [], <span class="built_in">dict</span>()<span class="string">&#x27;&#x27;&#x27;下标和token相互转换&#x27;&#x27;&#x27;</span></span><br><span class="line">        <span class="keyword">for</span> token <span class="keyword">in</span> uniq_tokens:</span><br><span class="line">            <span class="variable language_">self</span>.idx_to_token.append(token)</span><br><span class="line">            <span class="variable language_">self</span>.token_to_idx[token] = <span class="built_in">len</span>(<span class="variable language_">self</span>.idx_to_token) - <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(<span class="variable language_">self</span>.idx_to_token)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, tokens</span>):<span class="string">&#x27;&#x27;&#x27;token-&gt;index，返回index&#x27;&#x27;&#x27;</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> <span class="built_in">isinstance</span>(tokens, (<span class="built_in">list</span>, <span class="built_in">tuple</span>)):</span><br><span class="line">            <span class="keyword">return</span> <span class="variable language_">self</span>.token_to_idx.get(tokens, <span class="variable language_">self</span>.unk)</span><br><span class="line">        <span class="keyword">return</span> [<span class="variable language_">self</span>.__getitem__(token) <span class="keyword">for</span> token <span class="keyword">in</span> tokens]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">to_tokens</span>(<span class="params">self, indices</span>):<span class="string">&#x27;&#x27;&#x27;index-&gt;token，返回token&#x27;&#x27;&#x27;</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> <span class="built_in">isinstance</span>(indices, (<span class="built_in">list</span>, <span class="built_in">tuple</span>)):</span><br><span class="line">            <span class="keyword">return</span> <span class="variable language_">self</span>.idx_to_token[indices]</span><br><span class="line">        <span class="keyword">return</span> [<span class="variable language_">self</span>.idx_to_token[index] <span class="keyword">for</span> index <span class="keyword">in</span> indices]</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">count_corpus</span>(<span class="params">tokens</span>):  </span><br><span class="line">    <span class="string">&quot;&quot;&quot;统计标记的频率。&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(tokens) == <span class="number">0</span> <span class="keyword">or</span> <span class="built_in">isinstance</span>(tokens[<span class="number">0</span>], <span class="built_in">list</span>):</span><br><span class="line">        tokens = [token <span class="keyword">for</span> line <span class="keyword">in</span> tokens <span class="keyword">for</span> token <span class="keyword">in</span> line]</span><br><span class="line">    <span class="keyword">return</span> collections.Counter(tokens)</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure> ### 构建词汇表 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">vocab = Vocab(tokens)</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">list</span>(vocab.token_to_idx.items())[:<span class="number">10</span>])</span><br></pre></td></tr></table></figure> [('unk', 0), ('the', 1),
('i', 2), ('and', 3), ('of', 4), ('a', 5), ('to', 6), ('was', 7), ('in',
8), ('that', 9)] ### 将每一行文本转换成一个数字索引列表 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> [<span class="number">0</span>, <span class="number">10</span>]:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;words:&#x27;</span>, tokens[i])</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;indices:&#x27;</span>, vocab[tokens[i]])</span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;当编写_getitem_方法并包含在你的类中时，python解释器会在实例上使用方括号自动调用方法&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>
words: ['the', 'time', 'machine', 'by', 'h', 'g', 'wells'] indices: [1,
19, 50, 40, 2183, 2184, 400] words: ['twinkled', 'and', 'his',
'usually', 'pale', 'face', 'was', 'flushed', 'and', 'animated', 'the']
indices: [2186, 3, 25, 1044, 362, 113, 7, 1421, 3, 1045, 1] ###
将所有内容打包到<code>load_corpus_time_machine</code>函数中
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">load_corpus_time_machine</span>(<span class="params">max_tokens=-<span class="number">1</span></span>):  </span><br><span class="line">    <span class="string">&quot;&quot;&quot;返回时光机器数据集的标记索引列表和词汇表。&quot;&quot;&quot;</span></span><br><span class="line">    lines = read_time_machine()</span><br><span class="line">    tokens = tokenize(lines, <span class="string">&#x27;char&#x27;</span>)</span><br><span class="line">    vocab = Vocab(tokens)<span class="string">&#x27;&#x27;&#x27;对应字典&#x27;&#x27;&#x27;</span></span><br><span class="line">    corpus = [vocab[token] <span class="keyword">for</span> line <span class="keyword">in</span> tokens <span class="keyword">for</span> token <span class="keyword">in</span> line]<span class="string">&#x27;&#x27;&#x27;每一个单词(这里是字母)的数字&#x27;&#x27;&#x27;</span></span><br><span class="line">    <span class="keyword">if</span> max_tokens &gt; <span class="number">0</span>:</span><br><span class="line">        corpus = corpus[:max_tokens]</span><br><span class="line">    <span class="keyword">return</span> corpus, vocab</span><br><span class="line"></span><br><span class="line">corpus, vocab = load_corpus_time_machine()</span><br><span class="line"><span class="built_in">len</span>(corpus), <span class="built_in">len</span>(vocab)</span><br></pre></td></tr></table></figure> (170580, 28) 28=16+ukn+空格 ## 8.3 语言模型
1.给定文本序列x1-xT，语言模型的目标是估计联合概率p（x1-xT）
2.使用计数来建模 - 假设序列长度为2，<span
class="math inline">\(p(x,x&#39;)=p(x)p(x&#39;|x)=\frac{n(x)}{n}\frac{n(x,x&#39;)}{n(x)}\)</span>,n是语料库中的总词数，n（x），n（x，x'）是单个单词和连续单词对的出现次数
3.N元语法 - 当序列很长时，因为文本量不够大，很可能n（x1-xT）&lt;=1 -
使用马尔可夫假设 - 一元语法：<span
class="math inline">\(p(x_1,x_2,x_3,x_4)=p(x_1)p(x_2)p(x_3)p(x_4)=\frac{n(x_1)}{n}\frac{n(x_2)}{n}\frac{n(x_3)}{n}\frac{n(x_4)}{n}\)</span>
- 一元语法：<span
class="math inline">\(p(x_1,x_2,x_3,x_4)=p(x_1)p(x_2|x_1)p(x_3|x_2)p(x_4|x_3)=\frac{n(x_1)}{n}\frac{n(x_1,x_2)}{x_1}\frac{n(x_2,x_3)}{x_2}\frac{n(x_3,x_4)}{x_3}\)</span>
4.代码 1.F1:随机地生成一个小批量数据的特征和标签以供读取。
在随机采样中，每个样本都是在原始的长序列上任意捕获的子序列 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">seq_data_iter_random</span>(<span class="params">corpus, batch_size, num_steps<span class="string">&#x27;&#x27;&#x27;tau&#x27;&#x27;&#x27;</span></span>):  </span><br><span class="line">    <span class="string">&quot;&quot;&quot;使用随机抽样生成一个小批量子序列。&quot;&quot;&quot;</span></span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;不同于8.1中的遍历每个都需要用很多次，这个是把总长切成n份，然后一个epoch就n份都过一遍。每个epoch开始的起点都是从0~tau中随机取，这样份就不会重复&#x27;&#x27;&#x27;</span></span><br><span class="line">    corpus = corpus[random.randint(<span class="number">0</span>, num_steps - <span class="number">1</span>):]</span><br><span class="line">    num_subseqs = (<span class="built_in">len</span>(corpus) - <span class="number">1</span>) // num_steps</span><br><span class="line">    initial_indices = <span class="built_in">list</span>(<span class="built_in">range</span>(<span class="number">0</span>, num_subseqs * num_steps, num_steps))<span class="string">&#x27;&#x27;&#x27;每个子序列开始的下标&#x27;&#x27;&#x27;</span></span><br><span class="line">    random.shuffle(initial_indices)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">data</span>(<span class="params">pos</span>):</span><br><span class="line">        <span class="keyword">return</span> corpus[pos:pos + num_steps]</span><br><span class="line"></span><br><span class="line">    num_batches = num_subseqs // batch_size<span class="string">&#x27;&#x27;&#x27;在n段里面取batch&#x27;&#x27;&#x27;</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, batch_size * num_batches, batch_size):</span><br><span class="line">        initial_indices_per_batch = initial_indices[i:i + batch_size]<span class="string">&#x27;&#x27;&#x27;取了batch size个开始的下标&#x27;&#x27;&#x27;</span></span><br><span class="line">        X = [data(j) <span class="keyword">for</span> j <span class="keyword">in</span> initial_indices_per_batch]<span class="string">&#x27;&#x27;&#x27;生成batch size个段&#x27;&#x27;&#x27;</span></span><br><span class="line">        Y = [data(j + <span class="number">1</span>) <span class="keyword">for</span> j <span class="keyword">in</span> initial_indices_per_batch]</span><br><span class="line">        <span class="keyword">yield</span> torch.tensor(X), torch.tensor(Y)</span><br></pre></td></tr></table></figure>
2.F2:保证两个相邻的小批量中的子序列在原始序列上也是相邻的 ## 8.4
循环神经网络 1.![[Pasted image 20240807153154.png]] 更新隐藏状态：<span
class="math inline">\(h_t=\phi(W_{hh}h_{t-1}+W_{hx}x_{t-1}+b_h)\)</span>
(去掉<span class="math inline">\(W_{hh}h_{t-1}\)</span>就是MLP) <span
class="math inline">\(o_t=W_{ho}h_{t}+b_o\)</span> ![[Pasted image
20240807153448.png]] ![[Pasted image 20240807153946.png]] 2.困惑度
perplexity - 衡量一个语言模型的好坏可以用平均交叉熵<span
class="math inline">\(\pi=\frac{1}{n}\sum_{i=1}^{n}-logp(x_t|x_{t-1},...)\)</span>
其中p是语言模型预测的概率，xt是真实词 -
NLP使用困惑度exp（pi）来衡量，1表示完美，无穷大是最差情况 3.梯度裁剪 -
迭代中计算T个时间步上的梯度，在反向传播中产生长度为O（T）的矩阵乘法链，导致数值不稳定
- 梯度裁剪能有效预防梯度爆炸 - 如果梯度长度超过<span
class="math inline">\(\theta\)</span>,那么将拖回<span
class="math inline">\(\theta\)</span>：<span
class="math inline">\(g\leftarrow min(1,\frac{\theta}{\lVert
g\rVert})g\)</span> 4.其他应用 ![[Pasted image 20240807183941.png]]
5.由于在当前时间步中， 隐状态使用的定义与前一个时间步中使用的定义相同，
因此 <span
class="math inline">\(h_t=\phi(W_{hh}h_{t-1}+W_{hx}x_{t-1}+b_h)\)</span>的计算是循环的
## 8.5 循环神经网络的从零开始实现 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">%matplotlib inline</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line">batch_size, num_steps = <span class="number">32</span>, <span class="number">35</span></span><br><span class="line">train_iter, vocab = d2l.load_data_time_machine(batch_size, num_steps)</span><br></pre></td></tr></table></figure> ### 独热编码
给一个下标，用向量来表示 将每个索引映射为相互不同的单位向量：
假设词表中不同词元的数目为N（即<code>len(vocab)</code>），
词元索引的范围为0到N−1。 如果词元的索引是整数i，
那么我们将创建一个长度为N的全0向量， 并将第i处的元素设置为1。
此向量是原始词元的一个独热向量 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">F.one_hot(torch.tensor([<span class="number">0</span>, <span class="number">2</span>]), <span class="built_in">len</span>(vocab))</span><br></pre></td></tr></table></figure> tensor([[1, 0, 0, 0, 0, 0,
0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0,
0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
0, 0, 0]]) ### 小批量形状是(批量大小, 时间步数)
转置的目的：使我们能够更方便地通过最外层的维度，
一步一步地更新小批量数据的隐状态 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">X = torch.arange(<span class="number">10</span>).reshape((<span class="number">2</span>, <span class="number">5</span>))</span><br><span class="line">F.one_hot(X.T, <span class="number">28</span>).shape</span><br></pre></td></tr></table></figure> torch.Size([5, 2, 28])
### 初始化循环神经网络模型的模型参数 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">get_params</span>(<span class="params">vocab_size, num_hiddens, device</span>):</span><br><span class="line">    num_inputs = num_outputs = vocab_size</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">normal</span>(<span class="params">shape</span>):</span><br><span class="line">        <span class="keyword">return</span> torch.randn(size=shape, device=device) * <span class="number">0.01</span></span><br><span class="line"></span><br><span class="line">    W_xh = normal((num_inputs, num_hiddens))</span><br><span class="line">    W_hh = normal((num_hiddens, num_hiddens))</span><br><span class="line">    b_h = torch.zeros(num_hiddens, device=device)</span><br><span class="line">    W_hq = normal((num_hiddens, num_outputs))</span><br><span class="line">    b_q = torch.zeros(num_outputs, device=device)</span><br><span class="line">    params = [W_xh, W_hh, b_h, W_hq, b_q]</span><br><span class="line">    <span class="keyword">for</span> param <span class="keyword">in</span> params:</span><br><span class="line">        param.requires_grad_(<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">return</span> params</span><br></pre></td></tr></table></figure> ###
一个<code>init_rnn_state</code>函数在初始化时返回隐藏状态 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">init_rnn_state</span>(<span class="params">batch_size, num_hiddens, device</span>):</span><br><span class="line">    <span class="keyword">return</span> (torch.zeros((batch_size, num_hiddens), device=device),)</span><br></pre></td></tr></table></figure>
### 下面的<code>rnn</code>函数定义了如何在一个时间步计算隐藏状态和输出
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">rnn</span>(<span class="params">inputs, state, params</span>):</span><br><span class="line">    W_xh, W_hh, b_h, W_hq, b_q = params</span><br><span class="line">    H, = state</span><br><span class="line">    outputs = []</span><br><span class="line">    <span class="keyword">for</span> X <span class="keyword">in</span> inputs:</span><br><span class="line">        H = torch.tanh(torch.mm(X, W_xh) + torch.mm(H, W_hh) + b_h)</span><br><span class="line">        Y = torch.mm(H, W_hq) + b_q</span><br><span class="line">        outputs.append(Y)</span><br><span class="line">    <span class="keyword">return</span> torch.cat(outputs, dim=<span class="number">0</span>), (H,)</span><br></pre></td></tr></table></figure> ### 创建一个类来包装这些函数 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">RNNModelScratch</span>:  </span><br><span class="line">    <span class="string">&quot;&quot;&quot;从零开始实现的循环神经网络模型&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, vocab_size, num_hiddens, device, get_params,</span></span><br><span class="line"><span class="params">                 init_state, forward_fn</span>):</span><br><span class="line">        <span class="variable language_">self</span>.vocab_size, <span class="variable language_">self</span>.num_hiddens = vocab_size, num_hiddens</span><br><span class="line">        <span class="variable language_">self</span>.params = get_params(vocab_size, num_hiddens, device)</span><br><span class="line">        <span class="variable language_">self</span>.init_state, <span class="variable language_">self</span>.forward_fn = init_state, forward_fn</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__call__</span>(<span class="params">self, X, state</span>):</span><br><span class="line">        X = F.one_hot(X.T, <span class="variable language_">self</span>.vocab_size).<span class="built_in">type</span>(torch.float32)</span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.forward_fn(X, state, <span class="variable language_">self</span>.params)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">begin_state</span>(<span class="params">self, batch_size, device</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.init_state(batch_size, <span class="variable language_">self</span>.num_hiddens, device)</span><br></pre></td></tr></table></figure> ###
检查输出是否具有正确的形状 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">num_hiddens = <span class="number">512</span></span><br><span class="line">net = RNNModelScratch(<span class="built_in">len</span>(vocab), num_hiddens, d2l.try_gpu(), get_params,</span><br><span class="line">                      init_rnn_state, rnn)</span><br><span class="line">state = net.begin_state(X.shape[<span class="number">0</span>], d2l.try_gpu())</span><br><span class="line">Y, new_state = net(X.to(d2l.try_gpu()), state)</span><br><span class="line">Y.shape, <span class="built_in">len</span>(new_state), new_state[<span class="number">0</span>].shape</span><br></pre></td></tr></table></figure> (torch.Size([10, 28]), 1,
torch.Size([2, 512])) ###
首先定义预测函数来生成用户提供的<code>prefix</code>之后的新字符
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">predict_ch8</span>(<span class="params">prefix, num_preds, net, vocab, device</span>):  </span><br><span class="line">    <span class="string">&quot;&quot;&quot;在`prefix`后面生成新字符。&quot;&quot;&quot;</span></span><br><span class="line">    state = net.begin_state(batch_size=<span class="number">1</span>, device=device)</span><br><span class="line">    outputs = [vocab[prefix[<span class="number">0</span>]]]</span><br><span class="line">    get_input = <span class="keyword">lambda</span>: torch.tensor([outputs[-<span class="number">1</span>]], device=device).reshape(</span><br><span class="line">        (<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">    <span class="keyword">for</span> y <span class="keyword">in</span> prefix[<span class="number">1</span>:]:</span><br><span class="line">        _, state = net(get_input(), state)</span><br><span class="line">        outputs.append(vocab[y])</span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(num_preds):</span><br><span class="line">        y, state = net(get_input(), state)</span><br><span class="line">        outputs.append(<span class="built_in">int</span>(y.argmax(dim=<span class="number">1</span>).reshape(<span class="number">1</span>)))</span><br><span class="line">    <span class="keyword">return</span> <span class="string">&#x27;&#x27;</span>.join([vocab.idx_to_token[i] <span class="keyword">for</span> i <span class="keyword">in</span> outputs])</span><br><span class="line"></span><br><span class="line">predict_ch8(<span class="string">&#x27;time traveller &#x27;</span>, <span class="number">10</span>, net, vocab, d2l.try_gpu())</span><br></pre></td></tr></table></figure> ### 梯度裁剪 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">grad_clipping</span>(<span class="params">net, theta</span>):  </span><br><span class="line">    <span class="string">&quot;&quot;&quot;裁剪梯度。&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">isinstance</span>(net, nn.Module):</span><br><span class="line">        params = [p <span class="keyword">for</span> p <span class="keyword">in</span> net.parameters() <span class="keyword">if</span> p.requires_grad]</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        params = net.params</span><br><span class="line">    norm = torch.sqrt(<span class="built_in">sum</span>(torch.<span class="built_in">sum</span>((p.grad**<span class="number">2</span>)) <span class="keyword">for</span> p <span class="keyword">in</span> params))</span><br><span class="line">    <span class="keyword">if</span> norm &gt; theta:</span><br><span class="line">        <span class="keyword">for</span> param <span class="keyword">in</span> params:</span><br><span class="line">            param.grad[:] *= theta / norm</span><br></pre></td></tr></table></figure> ###
定义一个函数来训练只有一个迭代周期的模型 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">train_epoch_ch8</span>(<span class="params">net, train_iter, loss, updater, device, use_random_iter</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;训练模型一个迭代周期（定义见第8章）。&quot;&quot;&quot;</span></span><br><span class="line">    state, timer = <span class="literal">None</span>, d2l.Timer()</span><br><span class="line">    metric = d2l.Accumulator(<span class="number">2</span>)</span><br><span class="line">    <span class="keyword">for</span> X, Y <span class="keyword">in</span> train_iter:</span><br><span class="line">        <span class="keyword">if</span> state <span class="keyword">is</span> <span class="literal">None</span> <span class="keyword">or</span> use_random_iter:</span><br><span class="line">            state = net.begin_state(batch_size=X.shape[<span class="number">0</span>], device=device)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">isinstance</span>(net, nn.Module) <span class="keyword">and</span> <span class="keyword">not</span> <span class="built_in">isinstance</span>(state, <span class="built_in">tuple</span>):</span><br><span class="line">                state.detach_()</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="keyword">for</span> s <span class="keyword">in</span> state:</span><br><span class="line">                    s.detach_()</span><br><span class="line">        y = Y.T.reshape(-<span class="number">1</span>)</span><br><span class="line">        X, y = X.to(device), y.to(device)</span><br><span class="line">        y_hat, state = net(X, state)</span><br><span class="line">        l = loss(y_hat, y.long()).mean()</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">isinstance</span>(updater, torch.optim.Optimizer):</span><br><span class="line">            updater.zero_grad()</span><br><span class="line">            l.backward()</span><br><span class="line">            grad_clipping(net, <span class="number">1</span>)</span><br><span class="line">            updater.step()</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            l.backward()</span><br><span class="line">            grad_clipping(net, <span class="number">1</span>)</span><br><span class="line">            updater(batch_size=<span class="number">1</span>)</span><br><span class="line">        metric.add(l * y.numel(), y.numel())</span><br><span class="line">    <span class="keyword">return</span> math.exp(metric[<span class="number">0</span>] / metric[<span class="number">1</span>]), metric[<span class="number">1</span>] / timer.stop()</span><br></pre></td></tr></table></figure> ###
训练函数支持从零开始或使用高级API实现的循环神经网络模型 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">train_ch8</span>(<span class="params">net, train_iter, vocab, lr, num_epochs, device,</span></span><br><span class="line"><span class="params">              use_random_iter=<span class="literal">False</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;训练模型（定义见第8章）。&quot;&quot;&quot;</span></span><br><span class="line">    loss = nn.CrossEntropyLoss()</span><br><span class="line">    animator = d2l.Animator(xlabel=<span class="string">&#x27;epoch&#x27;</span>, ylabel=<span class="string">&#x27;perplexity&#x27;</span>,</span><br><span class="line">                            legend=[<span class="string">&#x27;train&#x27;</span>], xlim=[<span class="number">10</span>, num_epochs])</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">isinstance</span>(net, nn.Module):</span><br><span class="line">        updater = torch.optim.SGD(net.parameters(), lr)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        updater = <span class="keyword">lambda</span> batch_size: d2l.sgd(net.params, lr, batch_size)</span><br><span class="line">    predict = <span class="keyword">lambda</span> prefix: predict_ch8(prefix, <span class="number">50</span>, net, vocab, device)</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">        ppl, speed = train_epoch_ch8(net, train_iter, loss, updater, device,</span><br><span class="line">                                     use_random_iter)</span><br><span class="line">        <span class="keyword">if</span> (epoch + <span class="number">1</span>) % <span class="number">10</span> == <span class="number">0</span>:</span><br><span class="line">            <span class="built_in">print</span>(predict(<span class="string">&#x27;time traveller&#x27;</span>))</span><br><span class="line">            animator.add(epoch + <span class="number">1</span>, [ppl])</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;困惑度 <span class="subst">&#123;ppl:<span class="number">.1</span>f&#125;</span>, <span class="subst">&#123;speed:<span class="number">.1</span>f&#125;</span> 标记/秒 <span class="subst">&#123;<span class="built_in">str</span>(device)&#125;</span>&#x27;</span>)</span><br><span class="line">    <span class="built_in">print</span>(predict(<span class="string">&#x27;time traveller&#x27;</span>))</span><br><span class="line">    <span class="built_in">print</span>(predict(<span class="string">&#x27;traveller&#x27;</span>))</span><br></pre></td></tr></table></figure>
## 8.6 循环神经网络的简洁实现 # 第九章 现代循环神经网络 ## 9.1
门控循环单元GRU 1.不是所有观察值同等重要-&gt;只记住相关的观察需要 -
能关注的机制：更新门 - 能遗忘的机制：重置门 2.门 <span
class="math inline">\(\sigma\)</span>是sigmoid ![[Pasted image
20240810094730.png]] 候选隐藏状态 ![[Pasted image 20240810101658.png]]
Rt可以学习，sigmoid介于0-1之间，0就忘了过去的Ht，只和Xt有关，1就过去的Ht全留着，和正常的隐藏状态一样
![[Pasted image 20240810103722.png]]
Zt可以学习，sigmoid介于0-1之间，0就拿来候选Ht（调整过的，用了
Xt），1就直接用上一个Ht，和新输入Xt没有关系
3.门控循环单元具有以下两个显著特征： -
重置门有助于捕获序列中的短期依赖关系 -
更新门有助于捕获序列中的长期依赖关系 ## 9.2 长短期记忆网络LSTM 1. -
忘记门：将值朝0减少 - 输入门：决定是否忽略掉输入数据 -
输出门：决定是不是使用隐状态 2.![[Pasted image 20240810160325.png]]
![[Pasted image 20240810160348.png]] ![[Pasted image
20240810160512.png]] ![[Pasted image 20240810161453.png]]
只要输出门接近1，我们就能够有效地将所有记忆信息传递给预测部分，
而对于输出门接近0，我们只保留记忆元内的所有信息，而不需要更新隐状态。（只有隐状态才会传递到输出层，
而记忆元Ct不直接参与输出计算） ## 9.3 深度循环神经网络 ![[Pasted image
20240810171115.png]] ## 9.4 双向循环神经网络
1.RNN只看过去，但我们也可以看未来（完形填空） ![[Pasted image
20240810175227.png]] 2.总结： -
双向循环神经网络通过反向更新的隐藏层来利用反向时间信息 -
通常用来对序列抽取特征、填空，而不是预测未来（推理） ## 9.5
机器翻译与数据集 - 下载和预处理数据集 - 几个预处理步骤 - 词元化 - 词汇表
- 序列样本都有一个固定的长度 截断或填充文本序列 -
转换成小批量数据集用于训练 - 训练模型 ## 9.6 编码器-解码器架构 1.CNN -
编码器：将输入编程成中间表达形式（特征） - 解码器：将中间表示解码输出 -
![[Pasted image 20240811150033.png]] 2.RNN - 编码器：将文本表示成向量 -
解码器：向量表示成输出 - ![[Pasted image 20240811150238.png]] 3.架构 -
编码器处理输入 - 解码器处理输出 - ![[Pasted image 20240811163952.png]]
## 9.7 序列到序列学习seq2seq 1.seq2seq ![[Pasted image
20240811170434.png]] - 编码器是一个RNN，读取输入句子（可以是双向） -
解码器使用另外一个RNN来输出 2.细节 - 编码器是没有输出的RNN -
编码器最后时间步的隐藏状态用作解码器的初始隐藏状态 - ![[Pasted image
20240811170122.png]] 3.训练 训练时解码器使用目标句子作为输入 ![[Pasted
image 20240811170524.png]] 4.衡量生成的好坏：BLEU -
pn是预测中所有n-gram的精度 - e.g.:标签序列A B C D E F 和预测序列A B B C
D - p1=4/5，预测序列中五个只有第二个B没有出现 -
p2=3/4，预测序列中四个2元，BB没出现 - p3=1/3，p4=0 - BLEU： - <span
class="math inline">\(exp(min(0,1-\frac{len_{label}}{len_{pred}}))\prod_{n=1}^{k}p_{n}^{1/2^{n}}\)</span>
- if len label&gt; len
pred,后一项为负，那么exp负数就变成很小的数了，BLEU越大越好，最大就是exp0=1，所以min用来惩罚过短的预测
- 后面的连乘 pn&lt;1所以n越大这个乘积越大，即长匹配有高权重 ## 9.8
束搜索 1.贪心搜索 -
在seq2seq中使用了贪心搜索来预测序列，即将当前时刻预测概率最大的词输出 -
但贪心并不一定最优 - e.g.![[Pasted image 20240811210943.png]] 2.穷举搜索
- 最优算法：对所有可能的序列，计算概率，选取最好的 - 但计算上不可行
3.束搜索 - 保存最好的k个候选 -
在每个时刻，对每个候选新加一项（n种可能），在kn中选出最好的k个 -
e.g.![[Pasted image 20240811211711.png]] - ![[Pasted image
20240811211916.png]] -
长句子的概率越低，为了避免每次都只找短的，所以前面乘了一个东西，L是长度，越长，分母越大，这个玩意儿越小，而log出来是负数，所以整体值越大，相当于对长句子做了补偿</p>

    </div>

    
    
    

    <footer class="post-footer">

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2024/09/14/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/" rel="prev" title="计算机视觉">
                  <i class="fa fa-angle-left"></i> 计算机视觉
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/2024/09/14/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/" rel="next" title="自然语言处理">
                  自然语言处理 <i class="fa fa-angle-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">

  <div class="copyright">
    &copy; 
    <span itemprop="copyrightYear">2024</span>
    <span class="with-love">
      <i class="fa fa-heart"></i>
    </span>
    <span class="author" itemprop="copyrightHolder">SilkyFinish</span>
  </div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/pisces/" rel="noopener" target="_blank">NexT.Pisces</a>
  </div>

    </div>
  </footer>

  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>
  <div class="sidebar-dimmer"></div>
  <div class="back-to-top" role="button" aria-label="Back to top">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/sidebar.js"></script><script src="/js/next-boot.js"></script>

  






  




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"none","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>



</body>
</html>
